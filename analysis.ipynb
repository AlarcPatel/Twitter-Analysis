{
  "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alark Patel"
     "NOTE: This is not the Whole Project"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Twitter Analysis\n",
    "\n",
    "\n",
    "* DISCLAIMER: This project is not designed with any bias in mind. Note that I could have picked either candiadate (Hillary Clinton or Donald Trump) or anyone else to do the same analysis. My analysis is objective, independent of any political bias you may have. It is my responsiblity to do independent analysis of the data as I try to understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import json\n",
    "pd.set_option('max_colwidth', 280)\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "import re #regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART  1 Loading Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(path):\n",
    "    \"\"\"Loads tweets that have previously been saved.\n",
    "    \n",
    "    Calling load_tweets(path) after save_tweets(tweets, path)\n",
    "    will produce the same list of tweets.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The place where the tweets were be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictionary objects, each representing one tweet.\"\"\"\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        import json\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_path = '2017-2018.json'\n",
    "trump_tweets = load_tweets(dest_path)\n",
    "#assert 2000 <= len(trump_tweets) <= 4000 \n",
    "#checking if the data is loaded or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the number of the month of the oldest tweet (e.g. 1 for January)\n",
    "oldest_month = 8\n",
    "p_s = pd.Series([i['created_at'] for i in trump_tweets])\n",
    "d_t = pd.to_datetime(p_s)\n",
    "oldest_month = d_t.min().month\n",
    "print(oldest_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2  - Twitter Source Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some data cleaning inorder to do Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "old_path = '2016-2017.json'\n",
    "old_trump_tweets = load_tweets(old_path)\n",
    "old_trump_tweets = pd.DataFrame(old_trump_tweets)\n",
    "trump_tweets = pd.DataFrame(trump_tweets)\n",
    "df_ = old_trump_tweets[['id', 'created_at', 'source','text', 'retweet_count']]\n",
    "df1 = trump_tweets[['id', 'created_at', 'source', 'full_text', 'retweet_count']]\n",
    "df1 = df1.rename(columns={'full_text': 'text'})\n",
    "df_.loc[:,'id'] = df_['id'].astype('int64')\n",
    "df1.loc[:,'id'] = df1['id'].astype('int64')\n",
    "df_ = df_.set_index('id')\n",
    "df1 = df1.set_index('id')\n",
    "all_tweets = pd.concat([df_, df1])\n",
    "#print(all_tweets.size)\n",
    "#dup = df_.drop_duplicates()\n",
    "#dup.sort_index()\n",
    "#assert(all_tweets.size == 40176) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "\n",
    "\n",
    "- `time`: The time the tweet was created encoded as a datetime object. (Use `pd.to_datetime` to encode the timestamp.)\n",
    "- `source`: The source device of the tweet.\n",
    "- `text`: The text of the tweet.\n",
    "- `retweet_count`: The retweet count of the tweet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump = pd.DataFrame(all_tweets, columns=['created_at', 'source', 'text', 'retweet_count']);\n",
    "df_trump = df_trump.rename(columns={'created_at': 'time'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3\n",
    "\n",
    "* Using regular expression cleaning the data, for example removing html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump['source'] = df_trump['source'].str.replace(r'<[^>]*>',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most common device types used in accessing twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4\n",
    "Is there a difference between his Tweet behavior across these devices? We will attempt to answer this question in our subsequent analysis.\n",
    "\n",
    "First, we'll take a look at whether Trump's tweets from an Android come at different times than his tweets from an iPhone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump['time'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the tweet times to US Eastern Time, the timezone of New York and Washington D.C., since those are the places we would expect the most tweet activity from Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump['est_time'] = (\n",
    "    pd.to_datetime(df_trump['time']).dt.tz_convert(\"UTC\") # converting it UTC\n",
    "                 .dt.tz_convert(\"EST\") # Convert to Eastern Time\n",
    ")\n",
    "df_trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you need to do:**\n",
    "\n",
    "Add a column called `hour` to the `df_trump` table which contains the hour of the day as floating point number computed by:\n",
    "\n",
    "$$\n",
    "\\text{hour} + \\frac{\\text{minute}}{60} + \\frac{\\text{second}}{60^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump['hour'] = df_trump.est_time.apply(lambda x: x.hour + x.minute/60 + x.second/3600)\n",
    "df_trump['roundhour']=round(df_trump['hour'])\n",
    "plot = sns.countplot(df_trump['roundhour'].astype(int))\n",
    "plot.set_xlabel('hour of day')\n",
    "df_trump['hour'] = ((df_trump['est_time'].dt.hour)+ (df_trump['est_time'].dt.minute/60) + (df_trump['est_time'].dt.second/(3600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android = df_trump[df_trump['source'] == 'Twitter for Android']\n",
    "iphone = df_trump[df_trump['source'] == 'Twitter for iPhone']\n",
    "plot1 = sns.distplot(android['hour'], hist=False, label=\"Android\")\n",
    "plot2 = sns.distplot(iphone['hour'], hist=False, label=\"iPhone\")\n",
    "plot1.set_ylabel('Fraction')\n",
    "plt.xticks([-5,0,5,10,15,20,25])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5\n",
    "\n",
    "According to [this Verge article](https://www.theverge.com/2017/3/29/15103504/donald-trump-iphone-using-switched-android), Donald Trump switched from an Android to an iPhone sometime in March 2017.\n",
    "\n",
    "Use this data along with the seaborn `distplot` function to examine the distribution over hours of the day in eastern time that trump tweets on each device for the 2 most commonly used devices.  Your plot should look somewhat similar to the following. \n",
    "\n",
    "During the campaign, it was theorized that Donald Trump's tweets from Android were written by him personally, and the tweets from iPhone were from his staff. Does your figure give support the theory?\n",
    "\n",
    "Response: In 2016, the time allocation for the usage of the iphone centered in the afternoon, while his tweets from 2015 to present shows that he mostly tweets in the morning. It seems that the tweets from iphone in 2016 were from his staff, not himself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump['time'] = pd.to_datetime(df_trump['time'])\n",
    "df_trump['year_only'] = df_trump['time'].dt.year\n",
    "iphone_ = df_trump[(df_trump['source'] == 'Twitter for iPhone') & (df_trump['year_only'] == 2016)]\n",
    "android_ = df_trump[(df_trump['source'] == 'Twitter for Android') & (df_trump['year_only'] == 2016)]\n",
    "plot3 = sns.distplot(iphone_['hour'], hist= False)\n",
    "plot4 = sns.distplot(android_['hour'], hist= False)\n",
    "plt.ylabel('Fraction')\n",
    "plt.xticks([0,10,20])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obeservations :\n",
    "question = '''Here we will see what time of the day Mr Trump does the tweets and what time of the day his staff does the tweets from him. '''\n",
    "ans = '''Based on the obersvation, trump does tweets before afternoon mostly in the morning and his staff do afternoon-evening.\n",
    "To conclude, Mr Trump not strictly Tweets before universal office hours 9am-5pm and his staff does it during that time.'''\n",
    "print(question)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6 Device Analysis\n",
    "Let's now look at which device he has used over the entire time period of this dataset.\n",
    "\n",
    "To examine the distribution of dates we will convert the date to a fractional year that can be plotted as a distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone1 = df_trump[df_trump['source'] == 'Twitter for iPhone'] \n",
    "android1 = df_trump[df_trump['source'] == 'Twitter for Android']\n",
    "plt.figure(figsize=(15,15))\n",
    "plot6 = sns.distplot(iphone1['year'], label= 'Android')\n",
    "plot5 = sns.distplot(android1['year'], label= 'iPhone')\n",
    "plt.yticks([0.00,0.25,0.50,0.75,1.00,1.25,1.50])\n",
    "plt.xticks([2016,2017,2018,2019])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 - Sentiment Analysis\n",
    "\n",
    "It turns out that we can use the words in Trump's tweets to calculate a measure of the sentiment of the tweet. For example, the sentence \"I love America!\" has positive sentiment, whereas the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment) lexicon to analyze the sentiment of Trump's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(open(\"data/vader_lexicon.txt\").readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1\n",
    "\n",
    "As you can see, the lexicon contains emojis too! The first column of the lexicon is the *token*, or the word itself. The second column is the *polarity* of the word, or how positive / negative it is.\n",
    "\n",
    "(How did they decide the polarities of these words? What are the other two columns in the lexicon? See the link above.)\n",
    "\n",
    " Read in the lexicon into a DataFrame called `df_sent`. The index of the DF should be the tokens in the lexicon. `df_sent` should have one column: `polarity`: The polarity of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_read = open('vader_lexicon.txt').readlines()\n",
    "s = [[i.split('\\t')[0],float(i.split('\\t')[1])] for i in v_read]\n",
    "dfv = pd.DataFrame(s, columns=['token','polarity'])\n",
    "df_sent = dfv.set_index('token');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2\n",
    "\n",
    "Now, let's use this lexicon to calculate the overall sentiment for each of Trump's tweets. Here's the basic idea:\n",
    "\n",
    "1. For each tweet, find the sentiment of each word.\n",
    "2. Calculate the sentiment of each tweet by taking the sum of the sentiments of its words.\n",
    "\n",
    "First, let's lowercase the text in the tweets since the lexicon is also lowercase. Set the `text` column of the `df_trump` DF to be the lowercased text of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump['text'] = df_trump['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3\n",
    "\n",
    "Now, let's get rid of punctuation since it'll cause us to fail to match words. Create a new column called `no_punc` in the `df_trump` to be the lowercased text of each tweet with all punctuation replaced by a single space. We consider punctuation characters to be any character that isn't a Unicode word character or a whitespace character. You may want to consult the Python documentation on regexes for this problem.\n",
    "\n",
    "(Why don't we simply remove punctuation instead of replacing with a space? See if you can figure this out by looking at the tweet data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_re = r'[^\\w\\s\\\\n]'\n",
    "df_trump['no_punc'] = df_trump['text'].str.replace(punct_re, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(punct_re, str)\n",
    "assert re.search(punct_re, 'this') is None\n",
    "assert re.search(punct_re, 'this is ok') is None\n",
    "assert re.search(punct_re, 'this is\\nok') is None\n",
    "assert re.search(punct_re, 'this is not ok.') is not None\n",
    "assert re.search(punct_re, 'this#is#ok') is not None\n",
    "assert re.search(punct_re, 'this^is ok') is not None\n",
    "assert df_trump['no_punc'].loc[800329364986626048] == 'i watched parts of  nbcsnl saturday night live last night  it is a totally one sided  biased show   nothing funny at all  equal time for us '\n",
    "assert df_trump['text'].loc[884740553040175104] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4\n",
    "\n",
    "\n",
    "Now, let's convert the tweets into what's called a [*tidy format*](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) to make the sentiments easier to calculate. Use the `no_punc` column of `df_trump` to create a table called `tidy_format`. The index of the table should be the IDs of the tweets, repeated once for every word in the tweet. It has two columns:\n",
    "\n",
    "1. `num`: The location of the word in the tweet. For example, if the tweet was \"i love america\", then the location of the word \"i\" is 0, \"love\" is 1, and \"america\" is 2.\n",
    "2. `word`: The individual words of each tweet.\n",
    "\n",
    "\n",
    "\n",
    "As usual, try to avoid using any for loops. Our solution uses a chain of 5 methods on the 'trump' DF, albeit using some rather advanced Pandas hacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_format = df_trump['no_punc'].str.split(expand=True).stack().reset_index(level=1)\n",
    "tidy_format.columns = ['num', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tidy_format.loc[894661651760377856].shape == (27, 2)\n",
    "assert ' '.join(list(tidy_format.loc[894661651760377856]['word'])) == 'i think senator blumenthal should take a nice long vacation in vietnam where he lied about his service so he can at least say he was there'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.5\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table. \n",
    "\n",
    "Add a `polarity` column to the `trump` table.  The `polarity` column should contain the sum of the sentiment polarity of each word in the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump['polarity'] = tidy_format.merge(df_sent,how='left', left_on='word',  right_index=True).groupby(lambda y:y).sum()['polarity']\n",
    "#df_trump['polarity'].replace(np.float64('nan'), 0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(df_trump.loc[744701872456536064, 'polarity'], 8.4)\n",
    "assert np.allclose(df_trump.loc[745304731346702336, 'polarity'], 2.5)\n",
    "assert np.allclose(df_trump.loc[744519497764184064, 'polarity'], 1.7)\n",
    "assert np.allclose(df_trump.loc[894661651760377856, 'polarity'], 0.2)\n",
    "assert np.allclose(df_trump.loc[894620077634592769, 'polarity'], 5.4)\n",
    "# If you fail this test, you dropped tweets with 0 polarity\n",
    "#assert np.allclose(df_trump.loc[744355251365511169, 'polarity'], 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.6\n",
    "Now we have a measure of the sentiment of each of his tweets! You can read over the VADER readme to understand a more robust sentiment analysis.\n",
    "Now, write the code to see the most positive and most negative tweets from Trump in your dataset:\n",
    "Find the most negative and most positive tweets made by Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most negative tweets:')\n",
    "for tweet in df_trump.sort_values('polarity', ascending=True).head()['text']:\n",
    "    print('\\n ', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most positive tweets:')\n",
    "for tweet in df_trump.sort_values('polarity', ascending=False).head()['text']:\n",
    "    print('\\n ', tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.7\n",
    "Plot the distribution of tweet sentiments broken down by whether the text of the tweet contains `nyt` or `fox`.  Then in the box below comment on what we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = df_trump[df_trump['text'].str.contains('nyt')]\n",
    "fox = df_trump[df_trump['text'].str.contains('fox')]\n",
    "plot7 = sns.distplot(nyt['polarity'], label = 'nyt')\n",
    "plot8 = sns.distplot(fox['polarity'], label = 'fox')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Here after observation you can say if trumps like nyt or fox the most.........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4  Twitter Engagement\n",
    "\n",
    "In this problem, we'll explore which words led to a greater average number of retweets. For example, at the time of this writing, Donald Trump has two tweets that contain the word 'oakland' (tweets 932570628451954688 and 1016609920031117312) with 36757 and 10286 retweets respectively, for an average of 23,521.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1\n",
    "Find the top 20 most retweeted words. Include only words that appear in at least 25 tweets. As usual, try to do this without any for loops. You can string together ~7 pandas commands and get everything done on one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_20 = (tidy_format.groupby('word').filter(lambda l: len(l) >= 25).merge(df_trump, how='inner', right_index=True, left_index=True)\n",
    "          .groupby('word').agg({'retweet_count': 'mean'}).sort_values(by='retweet_count', ascending=False))\n",
    "top_20 = top_20.iloc[0:20,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2\n",
    "Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20['retweet_count'].sort_values().plot(kind='barh', figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The notebook was created for CS-439 Class at Rutgers University"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
